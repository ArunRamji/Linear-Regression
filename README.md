# Linear Regression ğŸ¤–ğŸ¤©

## Univariate Linear Regression

**Applications**
â€” Heart rate vs life expectancy
â€” running time vs cholesterol
â€” GDP vs Happiness rate

**Hypothesis h(x)**
h(x) = Î¸0 + Î¸1x     (or)  Î¸Tx
Î¸0 â€” Intercept Parameter
Î¸1â€” Cost function of x

**Compute Cost:**

J = (1 / (2*m) ) * sum(((X * theta)-y).^2)
 Gradient descend 
Gradient descend algorithmâ€™s job is to find the right parameter theta0 and theta1 which causes the very less J (compute cost) , in other words, global minimum optima.
Update rule as follows,
for iter = 1:num_iters
     error = (X * theta) - y; 
    temp0 = theta(1) - ( alpha /m ) * sum(error.* X(:,1));
    temp1 = theta(2) - ( alpha /m ) * sum(error.* X(:,2));
    theta = [temp0; temp1];
End

## Multivariate Linear Regression

 **Hypothesis h(x)**
 h(x) = Î¸0 + Î¸1x + Î¸2x + Î¸3x +â€¦â€¦â€¦. Î¸nx

Everything is more or less similar , but we will need to modify the update rule for adding few parameters based on number of the feature( x1,x2,x3..).

**Feature Normalisation**
It is a technique to make all the features in to similar magnitude to make the gradient descend more efficient . Few technique below
Feature value / max value 
Feature value - mean of all feature / max value
Feature value - mean of all feature / std.dev

**How to make sure gradient descend working fine?**

We can plot J(Î¸) and number of iteration , if J(Î¸) reduces significantly for each iteration then itâ€™s a positive signal that itâ€™s covering as expected.
If gradient descent not working then we may need to use smaller learning rate ğ›¼ to avoid the overshooting .
 ğ›¼ should be much smaller if iteration we using is huge number.







  
